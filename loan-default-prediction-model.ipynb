{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aniruddhapa/loan-default-prediction-model?scriptVersionId=188538567\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ab6849fc","metadata":{"papermill":{"duration":0.007769,"end_time":"2024-07-16T12:56:24.118976","exception":false,"start_time":"2024-07-16T12:56:24.111207","status":"completed"},"tags":[]},"source":["# Developing a Stable and Reliable Loan Default Prediction Model for Enhanced Financial Inclusion"]},{"cell_type":"markdown","id":"1dc30b00","metadata":{"papermill":{"duration":0.007404,"end_time":"2024-07-16T12:56:24.134028","exception":false,"start_time":"2024-07-16T12:56:24.126624","status":"completed"},"tags":[]},"source":["# Rational\n","\n","Consumer finance providers face significant challenges in accurately predicting loan default risk, especially for clients with little or no credit history. Traditional methods often fail to address the dynamic nature of client behavior, leading to unstable and frequently updated scorecards. A stable and reliable model is essential for making informed lending decisions that balance risk and accessibility.\n","\n","# Introduction\n","\n","This notebook aims to develop a predictive model to determine which clients are more likely to default on their loans. The goal is to provide a stable and reliable solution that remains effective over time, thus aiding consumer finance providers in making better lending decisions. This project is part of a competition hosted by Home Credit, an international consumer finance provider known for responsible lending practices and financial inclusion efforts.\n","\n","The competition runs from February 5, 2024, to May 28, 2024, and focuses on using data science to improve the prediction of loan default risk. \n","\n","A key evaluation criterion is the gini stability metric, which measures the model's predictive performance and stability over time.\n","\n","# Objective\n","\n","The primary objective of this project is to build a predictive model that accurately assesses the likelihood of loan default while maintaining stability over time. The model should:\n","\n","1. Utilize a robust data preprocessing and feature engineering pipeline.\n","2. Employ advanced machine learning techniques to maximize predictive accuracy.\n","3. Ensure stability in predictions across different time periods to minimize the need for frequent model updates.\n","\n","# Deliverables\n","\n","## Data Preprocessing and Feature Engineering:\n","\n","**1. Read and concatenate multiple datasets.**\n","\n","    * Set appropriate data types for columns.\n","    * Handle date features and filter out irrelevant or low-quality columns.\n","    * Engineer additional features to enrich the dataset.\n","\n","**2. Model Training and Evaluation:**\n","\n","    * Implement a cross-validation strategy using StratifiedGroupKFold to ensure stable performance evaluation.\n","    * Train a LightGBM classifier and utilize early stopping and logging callbacks.\n","    * Aggregate predictions from multiple cross-validation folds using a custom VotingModel.\n","\n","**3. Prediction and Submission:**\n","\n","    * Prepare the test dataset by setting the correct data types and indices.\n","    * Use the trained model to predict probabilities for the test set.\n","    * Create a submission file with case_id and predicted scores.\n","\n","**4. Model Stability Assessment:**\n","\n","    * Evaluate the model using the gini stability metric.\n","    * Ensure the model's predictions are stable over different weeks to avoid performance drop-offs.\n","\n","\n","By achieving these deliverables, the notebook will contribute to developing a reliable and stable predictive model that can help consumer finance providers make better lending decisions and potentially improve financial inclusion for individuals with limited credit history."]},{"cell_type":"markdown","id":"da6f6c11","metadata":{"papermill":{"duration":0.006717,"end_time":"2024-07-16T12:56:24.148054","exception":false,"start_time":"2024-07-16T12:56:24.141337","status":"completed"},"tags":[]},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"69d3ae58","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:24.164082Z","iopub.status.busy":"2024-07-16T12:56:24.163708Z","iopub.status.idle":"2024-07-16T12:56:28.691372Z","shell.execute_reply":"2024-07-16T12:56:28.689894Z"},"papermill":{"duration":4.538724,"end_time":"2024-07-16T12:56:28.693916","exception":false,"start_time":"2024-07-16T12:56:24.155192","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0.20.5\n"]}],"source":["import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","import os\n","import gc\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","print(pl.__version__)\n","from glob import glob\n","from pathlib import Path\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","\n"]},{"cell_type":"code","execution_count":2,"id":"ba504393","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.711589Z","iopub.status.busy":"2024-07-16T12:56:28.710939Z","iopub.status.idle":"2024-07-16T12:56:28.71775Z","shell.execute_reply":"2024-07-16T12:56:28.716301Z"},"papermill":{"duration":0.018867,"end_time":"2024-07-16T12:56:28.720658","exception":false,"start_time":"2024-07-16T12:56:28.701791","status":"completed"},"tags":[]},"outputs":[],"source":["ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","# ROOT            = Path(\"./input\")\n","\n","TRAIN_DIR       = ROOT / \"csv_files\" / \"train\"\n","TEST_DIR        = ROOT / \"csv_files\" / \"test\""]},{"cell_type":"markdown","id":"f9e5c7f1","metadata":{"papermill":{"duration":0.007212,"end_time":"2024-07-16T12:56:28.735384","exception":false,"start_time":"2024-07-16T12:56:28.728172","status":"completed"},"tags":[]},"source":["# Data Pre-Processing"]},{"cell_type":"markdown","id":"8a4cbebe","metadata":{"papermill":{"duration":0.007366,"end_time":"2024-07-16T12:56:28.750826","exception":false,"start_time":"2024-07-16T12:56:28.74346","status":"completed"},"tags":[]},"source":["# Pipeline Class for Changing the Data Types"]},{"cell_type":"code","execution_count":3,"id":"bcad7b71","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.768589Z","iopub.status.busy":"2024-07-16T12:56:28.768167Z","iopub.status.idle":"2024-07-16T12:56:28.781372Z","shell.execute_reply":"2024-07-16T12:56:28.780373Z"},"papermill":{"duration":0.024568,"end_time":"2024-07-16T12:56:28.783778","exception":false,"start_time":"2024-07-16T12:56:28.75921","status":"completed"},"tags":[]},"outputs":[],"source":["import polars as pl\n","from glob import glob\n","\n","class Pipeline:\n","    \n","    '''This decorator indicates that the method does not depend on the instance of the class and can be \n","    called on the class itself.\n","    The set_table_dtypes method is a static method designed to standardize the data types of columns \n","    in a Polars DataFrame (df). This method ensures that each column is cast to an appropriate data type \n","    based on specific rules and conventions.'''\n","    \n","    @staticmethod \n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","        return df\n","    \n","    '''The handle_dates function is designed to process date columns in a Polars DataFrame (df). It \n","    modifies columns whose names end with \"D\" by calculating the number of days between the dates in \n","    these columns and a reference date column (date_decision).'''\n","       \n","    @staticmethod\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","                df = df.with_columns(pl.col(col).dt.total_days())\n","        df = df.drop(\"date_decision\", \"MONTH\")\n","        return df\n","\n","    '''The filter_cols function is designed to clean and filter the columns of a Polars DataFrame (df). \n","    It performs two main tasks: removing columns with a high proportion of missing values and removing \n","    categorical columns with low or excessively high cardinality'''\n","    \n","    @staticmethod\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","\n","                if isnull > 0.95:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df\n"]},{"cell_type":"markdown","id":"ef484a26","metadata":{"papermill":{"duration":0.007949,"end_time":"2024-07-16T12:56:28.799315","exception":false,"start_time":"2024-07-16T12:56:28.791366","status":"completed"},"tags":[]},"source":["# Aggregator Class to summerise/transform the data by grouping and aggregating the values"]},{"cell_type":"code","execution_count":4,"id":"10c13437","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.816063Z","iopub.status.busy":"2024-07-16T12:56:28.815741Z","iopub.status.idle":"2024-07-16T12:56:28.827052Z","shell.execute_reply":"2024-07-16T12:56:28.825544Z"},"papermill":{"duration":0.022676,"end_time":"2024-07-16T12:56:28.829956","exception":false,"start_time":"2024-07-16T12:56:28.80728","status":"completed"},"tags":[]},"outputs":[],"source":["class Aggregator:\n","    \n","    '''Generates aggregation expressions for numeric columns (identified by suffixes \"P\" or \"A\"). \n","    It creates expressions to find the maximum value of these columns.'''\n","    \n","    @staticmethod\n","    def num_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        return expr_max\n","\n","    '''Generates aggregation expressions for date columns (identified by suffix \"D\"). \n","    It creates expressions to find the maximum value of these columns.'''\n","    \n","    @staticmethod\n","    def date_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        return expr_max\n","\n","    '''Generates aggregation expressions for string columns (identified by suffix \"M\"). \n","    It creates expressions to find the maximum value of these columns.'''\n","    \n","    @staticmethod\n","    def str_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        return expr_max\n","\n","    '''Generates aggregation expressions for other types of columns (identified by suffixes \"T\" or \"L\"). \n","    It creates expressions to find the maximum value of these columns.'''\n","    \n","    @staticmethod\n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        return expr_max\n","\n","    '''Generates aggregation expressions for columns related to grouping \n","    (columns containing \"num_group\"). It creates expressions to find the maximum value of these columns.'''\n","    \n","    @staticmethod\n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        return expr_max\n","\n","    '''Combines all the aggregation expressions from the other methods into a single list of expressions.'''\n","    \n","    @staticmethod\n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","        return exprs\n"]},{"cell_type":"markdown","id":"68c8feca","metadata":{"papermill":{"duration":0.006937,"end_time":"2024-07-16T12:56:28.844403","exception":false,"start_time":"2024-07-16T12:56:28.837466","status":"completed"},"tags":[]},"source":["# read_file method to read single .csv file"]},{"cell_type":"code","execution_count":5,"id":"7c765670","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.860928Z","iopub.status.busy":"2024-07-16T12:56:28.86011Z","iopub.status.idle":"2024-07-16T12:56:28.866285Z","shell.execute_reply":"2024-07-16T12:56:28.864751Z"},"papermill":{"duration":0.017605,"end_time":"2024-07-16T12:56:28.869168","exception":false,"start_time":"2024-07-16T12:56:28.851563","status":"completed"},"tags":[]},"outputs":[],"source":["def read_file(path, depth=None):\n","    df = pl.read_csv(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","\n","    if depth in [1, 2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","    return df"]},{"cell_type":"markdown","id":"a97a33e2","metadata":{"papermill":{"duration":0.006992,"end_time":"2024-07-16T12:56:28.884053","exception":false,"start_time":"2024-07-16T12:56:28.877061","status":"completed"},"tags":[]},"source":["# read_files method to read multiple .csv files"]},{"cell_type":"code","execution_count":6,"id":"24258b9d","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.90121Z","iopub.status.busy":"2024-07-16T12:56:28.900842Z","iopub.status.idle":"2024-07-16T12:56:28.907672Z","shell.execute_reply":"2024-07-16T12:56:28.906519Z"},"papermill":{"duration":0.017944,"end_time":"2024-07-16T12:56:28.909807","exception":false,"start_time":"2024-07-16T12:56:28.891863","status":"completed"},"tags":[]},"outputs":[],"source":["def read_files(regex_path, depth=None):\n","    chunks = []\n","    for path in glob(str(regex_path)):\n","        chunks.append(pl.read_csv(path).pipe(Pipeline.set_table_dtypes))\n","    df = pl.concat(chunks, how=\"vertical_relaxed\")\n","    if depth in [1, 2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","    return df"]},{"cell_type":"markdown","id":"45946b8a","metadata":{"papermill":{"duration":0.007461,"end_time":"2024-07-16T12:56:28.924583","exception":false,"start_time":"2024-07-16T12:56:28.917122","status":"completed"},"tags":[]},"source":["# Feature Engineering"]},{"cell_type":"markdown","id":"b2c582bb","metadata":{"papermill":{"duration":0.007866,"end_time":"2024-07-16T12:56:28.939767","exception":false,"start_time":"2024-07-16T12:56:28.931901","status":"completed"},"tags":[]},"source":["### Feature Enggineering methods for creating new features and merging dataframes"]},{"cell_type":"code","execution_count":7,"id":"5346dd36","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.956182Z","iopub.status.busy":"2024-07-16T12:56:28.955807Z","iopub.status.idle":"2024-07-16T12:56:28.964983Z","shell.execute_reply":"2024-07-16T12:56:28.963274Z"},"papermill":{"duration":0.020357,"end_time":"2024-07-16T12:56:28.967694","exception":false,"start_time":"2024-07-16T12:56:28.947337","status":"completed"},"tags":[]},"outputs":[],"source":["'''This feature_eng function is designed to perform feature engineering on a base DataFrame (df_base) along with additional DataFrames (depth_0, depth_1, and depth_2). Here's a breakdown of what it does:\n","\n","Date Features Addition:\n","\n","It adds two new columns to df_base: month_decision and weekday_decision.\n","These columns are derived from the date_decision column, capturing the month and weekday information.\n","\n","Joining Additional DataFrames:\n","\n","It iterates over the lists depth_0, depth_1, and depth_2, which contain additional DataFrames to be joined with df_base.\n","For each DataFrame in these lists, it performs a left join with df_base on the case_id column.\n","It adds a suffix to the column names of the joined DataFrames to differentiate them from existing columns in df_base.\n","\n","Date Handling:\n","\n","After all joins are performed, it applies the Pipeline.handle_dates method static method defined above\n","to handle date-related columns in the resulting DataFrame.\n","\n","Return Resulting DataFrame:\n","\n","The resulting DataFrame, after all feature engineering steps, is returned.'''\n","\n","\n","def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    df_base = (\n","        df_base\n","        .with_columns(\n","            month_decision = pl.col(\"date_decision\").dt.month(),\n","            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","        )\n","    )\n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    return df_base\n","\n","\n","''' to_pandas method converts Polars Dataframe to Pandas Dataframe.It identifies columns of type object as \n","categorical if cat_cols is not provided. It converts the identified categorical columns to the \n","category data type in Pandas.Finally, it returns the converted DataFrame and the list of categorical columns.'''\n","\n","def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols"]},{"cell_type":"markdown","id":"3658321c","metadata":{"papermill":{"duration":0.007059,"end_time":"2024-07-16T12:56:28.982055","exception":false,"start_time":"2024-07-16T12:56:28.974996","status":"completed"},"tags":[]},"source":["# Organizing and Loading Multiple Train and Test Datasets into data_store"]},{"cell_type":"code","execution_count":8,"id":"0fc045c5","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:56:28.998267Z","iopub.status.busy":"2024-07-16T12:56:28.997903Z","iopub.status.idle":"2024-07-16T12:57:09.482759Z","shell.execute_reply":"2024-07-16T12:57:09.48142Z"},"papermill":{"duration":40.495756,"end_time":"2024-07-16T12:57:09.485083","exception":false,"start_time":"2024-07-16T12:56:28.989327","status":"completed"},"tags":[]},"outputs":[],"source":["data_store = {\n","    \"df_base\": read_file(TRAIN_DIR / \"train_base.csv\"),\n","    \"depth_0\": [\n","        read_file(TRAIN_DIR / \"train_static_cb_0.csv\"),\n","        read_files(TRAIN_DIR / \"train_static_0_*.csv\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.csv\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.csv\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.csv\", 2),\n","    ]\n","}"]},{"cell_type":"code","execution_count":9,"id":"6cb7d948","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:09.501369Z","iopub.status.busy":"2024-07-16T12:57:09.500987Z","iopub.status.idle":"2024-07-16T12:57:16.089668Z","shell.execute_reply":"2024-07-16T12:57:16.087911Z"},"papermill":{"duration":6.599224,"end_time":"2024-07-16T12:57:16.091815","exception":false,"start_time":"2024-07-16T12:57:09.492591","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape:\t (1526659, 376)\n"]}],"source":["df_train = feature_eng(**data_store) # Unpacking the contents of the dictionary using ** Operator\n","print(\"train data shape:\\t\", df_train.shape)"]},{"cell_type":"code","execution_count":10,"id":"79c016b8","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:16.108533Z","iopub.status.busy":"2024-07-16T12:57:16.108158Z","iopub.status.idle":"2024-07-16T12:57:16.677617Z","shell.execute_reply":"2024-07-16T12:57:16.676317Z"},"papermill":{"duration":0.580571,"end_time":"2024-07-16T12:57:16.679978","exception":false,"start_time":"2024-07-16T12:57:16.099407","status":"completed"},"tags":[]},"outputs":[],"source":["data_store = {\n","    \"df_base\": read_file(TEST_DIR / \"test_base.csv\"),\n","    \"depth_0\": [\n","        read_file(TEST_DIR / \"test_static_cb_0.csv\"),\n","        read_files(TEST_DIR / \"test_static_0_*.csv\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TEST_DIR / \"test_applprev_1_*.csv\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_a_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_b_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_c_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_credit_bureau_b_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_other_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_person_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_deposit_1.csv\", 1),\n","        read_file(TEST_DIR / \"test_debitcard_1.csv\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TEST_DIR / \"test_credit_bureau_b_2.csv\", 2),\n","    ]\n","}"]},{"cell_type":"code","execution_count":11,"id":"38f9a1a5","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:16.696353Z","iopub.status.busy":"2024-07-16T12:57:16.695946Z","iopub.status.idle":"2024-07-16T12:57:16.730473Z","shell.execute_reply":"2024-07-16T12:57:16.728625Z"},"papermill":{"duration":0.046003,"end_time":"2024-07-16T12:57:16.733379","exception":false,"start_time":"2024-07-16T12:57:16.687376","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["test data shape:\t (10, 375)\n"]}],"source":["df_test = feature_eng(**data_store) # Unpacking the contents of the dictionary using ** Operator\n","print(\"test data shape:\\t\", df_test.shape)"]},{"cell_type":"markdown","id":"2060c6a5","metadata":{"papermill":{"duration":0.007235,"end_time":"2024-07-16T12:57:16.748196","exception":false,"start_time":"2024-07-16T12:57:16.740961","status":"completed"},"tags":[]},"source":["# Train and Test Dataset"]},{"cell_type":"code","execution_count":12,"id":"312d3dc4","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:16.764824Z","iopub.status.busy":"2024-07-16T12:57:16.764443Z","iopub.status.idle":"2024-07-16T12:57:19.119526Z","shell.execute_reply":"2024-07-16T12:57:19.118213Z"},"papermill":{"duration":2.366469,"end_time":"2024-07-16T12:57:19.122164","exception":false,"start_time":"2024-07-16T12:57:16.755695","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape:\t (1526659, 260)\n","test data shape:\t (10, 259)\n"]}],"source":["df_train = df_train.pipe(Pipeline.filter_cols)\n","df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n","\n","print(\"train data shape:\\t\", df_train.shape)\n","print(\"test data shape:\\t\", df_test.shape)"]},{"cell_type":"code","execution_count":13,"id":"5eeb8945","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:19.139014Z","iopub.status.busy":"2024-07-16T12:57:19.138665Z","iopub.status.idle":"2024-07-16T12:57:29.077376Z","shell.execute_reply":"2024-07-16T12:57:29.075603Z"},"papermill":{"duration":9.950023,"end_time":"2024-07-16T12:57:29.079828","exception":false,"start_time":"2024-07-16T12:57:19.129805","status":"completed"},"tags":[]},"outputs":[],"source":["df_train, cat_cols = to_pandas(df_train)\n","df_test, cat_cols = to_pandas(df_test, cat_cols)"]},{"cell_type":"markdown","id":"c7673da4","metadata":{"execution":{"iopub.execute_input":"2024-05-20T20:47:09.358919Z","iopub.status.busy":"2024-05-20T20:47:09.358517Z","iopub.status.idle":"2024-05-20T20:47:09.569362Z","shell.execute_reply":"2024-05-20T20:47:09.568057Z","shell.execute_reply.started":"2024-05-20T20:47:09.358889Z"},"papermill":{"duration":0.007556,"end_time":"2024-07-16T12:57:29.095393","exception":false,"start_time":"2024-07-16T12:57:29.087837","status":"completed"},"tags":[]},"source":["## del data_store\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":14,"id":"b9ee3a02","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:29.112183Z","iopub.status.busy":"2024-07-16T12:57:29.111862Z","iopub.status.idle":"2024-07-16T12:57:29.118599Z","shell.execute_reply":"2024-07-16T12:57:29.117126Z"},"papermill":{"duration":0.017962,"end_time":"2024-07-16T12:57:29.121258","exception":false,"start_time":"2024-07-16T12:57:29.103296","status":"completed"},"tags":[]},"outputs":[],"source":["class VotingModel(BaseEstimator, RegressorMixin):\n","    def __init__(self, estimators):\n","        super().__init__()\n","        self.estimators = estimators\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def predict(self, X):\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","\n","    def predict_proba(self, X):\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)"]},{"cell_type":"code","execution_count":15,"id":"15bbbc54","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:29.141203Z","iopub.status.busy":"2024-07-16T12:57:29.140887Z","iopub.status.idle":"2024-07-16T12:57:29.561922Z","shell.execute_reply":"2024-07-16T12:57:29.560711Z"},"papermill":{"duration":0.432913,"end_time":"2024-07-16T12:57:29.564422","exception":false,"start_time":"2024-07-16T12:57:29.131509","status":"completed"},"tags":[]},"outputs":[],"source":["X = df_train.drop(columns=[\"target\", \"case_id\",\"WEEK_NUM\"])\n","y = df_train[\"target\"]\n","weeks = df_train[\"WEEK_NUM\"]"]},{"cell_type":"code","execution_count":16,"id":"b76a413e","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:29.588458Z","iopub.status.busy":"2024-07-16T12:57:29.587688Z","iopub.status.idle":"2024-07-16T12:57:29.599416Z","shell.execute_reply":"2024-07-16T12:57:29.598111Z"},"papermill":{"duration":0.028229,"end_time":"2024-07-16T12:57:29.60203","exception":false,"start_time":"2024-07-16T12:57:29.573801","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["\"import optuna\\nfrom sklearn.model_selection import cross_validate\\nfrom lightgbm import LGBMClassifier\\n\\ndef objective(trial):\\n    max_depth = trial.suggest_int('max_depth', 3, 30)\\n    n_estimators = trial.suggest_int('n_estimators', 1, 1000)\\n    gamma = trial.suggest_float('gamma', 0, 1)\\n    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\\n    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\\n    min_child_weight = trial.suggest_int('min_child_weight', 0, 10)\\n    subsample = trial.suggest_float('subsample', 0, 1)\\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0, 1)\\n    learning_rate = trial.suggest_float('learning_rate', 0, 1)\\n    \\n#     print('Training the model with', X.shape[1], 'features')\\n    \\n#       LightGBM\\n    params = {'learning_rate': learning_rate,\\n              'n_estimators': n_estimators,\\n              'max_depth': max_depth,\\n              'lambda_l1': reg_alpha,\\n              'lambda_l2': reg_lambda,\\n              'colsample_bytree': colsample_bytree, \\n              'subsample': subsample,    \\n              'min_child_samples': min_child_weight,\\n              'class_weight': 'balanced'}\\n    \\n    clf = LGBMClassifier(**params, verbose = -1, verbosity = -1)\\n    \\n    cv_results = cross_validate(clf,X,y, cv=5, scoring='accuracy')\\n    \\n    validation_score = np.mean(cv_results['test_score'])\\n    \\n    return validation_score\""]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["'''import optuna\n","from sklearn.model_selection import cross_validate\n","from lightgbm import LGBMClassifier\n","\n","def objective(trial):\n","    max_depth = trial.suggest_int('max_depth', 3, 30)\n","    n_estimators = trial.suggest_int('n_estimators', 1, 1000)\n","    gamma = trial.suggest_float('gamma', 0, 1)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\n","    min_child_weight = trial.suggest_int('min_child_weight', 0, 10)\n","    subsample = trial.suggest_float('subsample', 0, 1)\n","    colsample_bytree = trial.suggest_float('colsample_bytree', 0, 1)\n","    learning_rate = trial.suggest_float('learning_rate', 0, 1)\n","    \n","#     print('Training the model with', X.shape[1], 'features')\n","    \n","#       LightGBM\n","    params = {'learning_rate': learning_rate,\n","              'n_estimators': n_estimators,\n","              'max_depth': max_depth,\n","              'lambda_l1': reg_alpha,\n","              'lambda_l2': reg_lambda,\n","              'colsample_bytree': colsample_bytree, \n","              'subsample': subsample,    \n","              'min_child_samples': min_child_weight,\n","              'class_weight': 'balanced'}\n","    \n","    clf = LGBMClassifier(**params, verbose = -1, verbosity = -1)\n","    \n","    cv_results = cross_validate(clf,X,y, cv=5, scoring='accuracy')\n","    \n","    validation_score = np.mean(cv_results['test_score'])\n","    \n","    return validation_score'''"]},{"cell_type":"code","execution_count":17,"id":"67072cfb","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:29.620349Z","iopub.status.busy":"2024-07-16T12:57:29.619968Z","iopub.status.idle":"2024-07-16T12:57:29.62879Z","shell.execute_reply":"2024-07-16T12:57:29.626934Z"},"papermill":{"duration":0.020952,"end_time":"2024-07-16T12:57:29.631506","exception":false,"start_time":"2024-07-16T12:57:29.610554","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'study = optuna.create_study(direction=\"maximize\")\\nstudy.optimize(objective, n_trials= 2)'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["'''study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials= 2)'''"]},{"cell_type":"markdown","id":"0ba6276f","metadata":{"papermill":{"duration":0.007672,"end_time":"2024-07-16T12:57:29.64739","exception":false,"start_time":"2024-07-16T12:57:29.639718","status":"completed"},"tags":[]},"source":["# Cross Validation to assess the performance of an LightGBM classifier\n","\n","This following code snippet is performing cross-validation using the StratifiedGroupKFold technique. Here's a breakdown of what it's doing:\n","\n","1. **StratifiedGroupKFold(n_splits=5, shuffle=False):** This initializes a StratifiedGroupKFold object with 5 splits and shuffle set to False, ensuring that each fold maintains the same class distribution and group membership.\n","\n","2. **fitted_models = [] and cv_scores = []:** These lists will store the trained models and the corresponding cross-validation scores, respectively.\n","\n","3. The loop iterates over each fold generated by the cross-validator **(cv.split(X, y, groups=weeks)). Within each iteration:**\n","\n","    * It splits the data into training and validation sets **(X_train, y_train, X_valid, y_valid)**     based on the current fold indices.\n","    * Trains an LightGBM classifier **(lgb.LGBMClassifier())** on the training data.\n","    * Evaluates the model on the validation set using the AUC score.\n","    * Appends the trained model to *fitted_models* and the AUC score to **cv_scores.**\n","\n","\n","4. Finally, it creates a VotingModel instance with the fitted models and prints the cross-validation AUC scores."]},{"cell_type":"code","execution_count":18,"id":"e71d2412","metadata":{"execution":{"iopub.execute_input":"2024-07-16T12:57:29.664948Z","iopub.status.busy":"2024-07-16T12:57:29.664611Z","iopub.status.idle":"2024-07-16T13:15:44.412185Z","shell.execute_reply":"2024-07-16T13:15:44.409072Z"},"papermill":{"duration":1094.760451,"end_time":"2024-07-16T13:15:44.415715","exception":false,"start_time":"2024-07-16T12:57:29.655264","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Valid week range:  (3, 90)\n","[LightGBM] [Info] Number of positive: 37755, number of negative: 1183301\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.390948 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 30534\n","[LightGBM] [Info] Number of data points in the train set: 1221056, number of used features: 256\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030920 -> initscore=-3.444945\n","[LightGBM] [Info] Start training from score -3.444945\n","Training until validation scores don't improve for 50 rounds\n","[50]\tvalid_0's binary_logloss: 0.124105\n","[100]\tvalid_0's binary_logloss: 0.122318\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.122318\n","Valid week range:  (0, 89)\n","[LightGBM] [Info] Number of positive: 38743, number of negative: 1182495\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.673172 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 30631\n","[LightGBM] [Info] Number of data points in the train set: 1221238, number of used features: 255\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031724 -> initscore=-3.418432\n","[LightGBM] [Info] Start training from score -3.418432\n","Training until validation scores don't improve for 50 rounds\n","[50]\tvalid_0's binary_logloss: 0.115352\n","[100]\tvalid_0's binary_logloss: 0.113635\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.113635\n","Valid week range:  (2, 91)\n","[LightGBM] [Info] Number of positive: 38570, number of negative: 1182654\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.675992 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 30563\n","[LightGBM] [Info] Number of data points in the train set: 1221224, number of used features: 255\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031583 -> initscore=-3.423042\n","[LightGBM] [Info] Start training from score -3.423042\n","Training until validation scores don't improve for 50 rounds\n","[50]\tvalid_0's binary_logloss: 0.116199\n","[100]\tvalid_0's binary_logloss: 0.114296\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.114296\n","Valid week range:  (1, 85)\n","[LightGBM] [Info] Number of positive: 38360, number of negative: 1183018\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.555038 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 30555\n","[LightGBM] [Info] Number of data points in the train set: 1221378, number of used features: 255\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031407 -> initscore=-3.428809\n","[LightGBM] [Info] Start training from score -3.428809\n","Training until validation scores don't improve for 50 rounds\n","[50]\tvalid_0's binary_logloss: 0.118231\n","[100]\tvalid_0's binary_logloss: 0.116328\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.116328\n","Valid week range:  (5, 88)\n","[LightGBM] [Info] Number of positive: 38548, number of negative: 1183192\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.598851 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 30551\n","[LightGBM] [Info] Number of data points in the train set: 1221740, number of used features: 256\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031552 -> initscore=-3.424067\n","[LightGBM] [Info] Start training from score -3.424067\n","Training until validation scores don't improve for 50 rounds\n","[50]\tvalid_0's binary_logloss: 0.117203\n","[100]\tvalid_0's binary_logloss: 0.115412\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.115412\n","CV AUC scores:  [0.8215086859361433, 0.8219322554244908, 0.8276391444915387, 0.8283419290944334, 0.8231817308018138]\n"]}],"source":["cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n","\n","\n","fitted_models = []\n","cv_scores = []\n","\n","\n","for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n","    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","\n","    print(\"Valid week range: \", (weeks.iloc[idx_valid].min(), weeks.iloc[idx_valid].max()))\n","\n","    model = lgb.LGBMClassifier()\n","    model.fit(\n","        X_train, y_train,\n","        eval_set=[(X_valid, y_valid)],\n","        callbacks=[lgb.log_evaluation(50), lgb.early_stopping(50)]\n","    )\n","\n","    fitted_models.append(model)\n","\n","\n","    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores.append(auc_score)\n","\n","model = VotingModel(fitted_models)\n","print(\"CV AUC scores: \", cv_scores)"]},{"cell_type":"code","execution_count":19,"id":"eb530d48","metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:15:44.443764Z","iopub.status.busy":"2024-07-16T13:15:44.443138Z","iopub.status.idle":"2024-07-16T13:15:44.685548Z","shell.execute_reply":"2024-07-16T13:15:44.683517Z"},"papermill":{"duration":0.259818,"end_time":"2024-07-16T13:15:44.688527","exception":false,"start_time":"2024-07-16T13:15:44.428709","status":"completed"},"tags":[]},"outputs":[],"source":["X_test = df_test.drop(columns=[\"WEEK_NUM\"])\n","X_test = X_test.set_index(\"case_id\")\n","\n","X_test[['pmtcount_693L', 'pmtscount_423L', 'deferredmnthsnum_166L', 'max_credacc_transactions_402L']] = X_test[['pmtcount_693L', 'pmtscount_423L', 'deferredmnthsnum_166L', 'max_credacc_transactions_402L']].astype(float)\n","\n","lgb_pred = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)"]},{"cell_type":"code","execution_count":20,"id":"2a86684f","metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:15:44.822634Z","iopub.status.busy":"2024-07-16T13:15:44.822178Z","iopub.status.idle":"2024-07-16T13:15:44.850727Z","shell.execute_reply":"2024-07-16T13:15:44.849759Z"},"papermill":{"duration":0.045525,"end_time":"2024-07-16T13:15:44.853263","exception":false,"start_time":"2024-07-16T13:15:44.807738","status":"completed"},"tags":[]},"outputs":[],"source":["df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")\n","\n","df_subm[\"score\"] = lgb_pred"]},{"cell_type":"code","execution_count":21,"id":"e80d8d2d","metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:15:44.87717Z","iopub.status.busy":"2024-07-16T13:15:44.876123Z","iopub.status.idle":"2024-07-16T13:15:44.883717Z","shell.execute_reply":"2024-07-16T13:15:44.882393Z"},"papermill":{"duration":0.022474,"end_time":"2024-07-16T13:15:44.886133","exception":false,"start_time":"2024-07-16T13:15:44.863659","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Check null:  False\n"]}],"source":["print(\"Check null: \", df_subm[\"score\"].isnull().any())\n"]},{"cell_type":"code","execution_count":22,"id":"7befeac4","metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:15:44.909389Z","iopub.status.busy":"2024-07-16T13:15:44.908975Z","iopub.status.idle":"2024-07-16T13:15:44.929177Z","shell.execute_reply":"2024-07-16T13:15:44.928294Z"},"papermill":{"duration":0.034786,"end_time":"2024-07-16T13:15:44.931742","exception":false,"start_time":"2024-07-16T13:15:44.896956","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","    </tr>\n","    <tr>\n","      <th>case_id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>57543</th>\n","      <td>0.017160</td>\n","    </tr>\n","    <tr>\n","      <th>57549</th>\n","      <td>0.027295</td>\n","    </tr>\n","    <tr>\n","      <th>57551</th>\n","      <td>0.006489</td>\n","    </tr>\n","    <tr>\n","      <th>57552</th>\n","      <td>0.012122</td>\n","    </tr>\n","    <tr>\n","      <th>57569</th>\n","      <td>0.067675</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            score\n","case_id          \n","57543    0.017160\n","57549    0.027295\n","57551    0.006489\n","57552    0.012122\n","57569    0.067675"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df_subm.head()"]},{"cell_type":"code","execution_count":23,"id":"40e3ccf6","metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:15:44.956018Z","iopub.status.busy":"2024-07-16T13:15:44.95562Z","iopub.status.idle":"2024-07-16T13:15:44.96802Z","shell.execute_reply":"2024-07-16T13:15:44.96692Z"},"papermill":{"duration":0.02813,"end_time":"2024-07-16T13:15:44.970846","exception":false,"start_time":"2024-07-16T13:15:44.942716","status":"completed"},"tags":[]},"outputs":[],"source":["df_subm.to_csv(\"submission.csv\")"]},{"cell_type":"code","execution_count":null,"id":"4339d73c","metadata":{"papermill":{"duration":0.009852,"end_time":"2024-07-16T13:15:44.991334","exception":false,"start_time":"2024-07-16T13:15:44.981482","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1165.199672,"end_time":"2024-07-16T13:15:46.631933","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-16T12:56:21.432261","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}